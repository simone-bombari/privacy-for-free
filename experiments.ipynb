{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "254d3fd7-fa03-4b26-bec3-25db3bed89fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "992f9daf-4134-455c-bb1c-24dc4b912190",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = relu\n",
    "dev_phi = dev_relu\n",
    "\n",
    "fmap = 'rf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60a7f63e-ff0e-4206-8a34-2f38ecb456df",
   "metadata": {},
   "outputs": [],
   "source": [
    "ks = [10000]\n",
    "d = 1000\n",
    "Ns = [3000]\n",
    "Nt = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "547c3b9e-37d1-48ec-ac00-aa2f4e1405a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "testr = []\n",
    "test = []\n",
    "\n",
    "\n",
    "for N in Ns:\n",
    "    for k in ks:\n",
    "    # Dataset\n",
    "       \n",
    "        dy = d // 2\n",
    "        dx = d - dy\n",
    "\n",
    "        u = np.random.randn(dx) / np.sqrt(dx)\n",
    "\n",
    "        X = np.random.randn(N, dx)\n",
    "        Y = np.random.randn(N, dy)\n",
    "        Z = np.concatenate((X, Y), axis=1)  # training data\n",
    "\n",
    "        G = np.sign(X @ u)\n",
    "\n",
    "        Xa = np.random.randn(N, dx)\n",
    "        Za = np.concatenate((Xa, Y), axis=1)  # attack data\n",
    "\n",
    "        Xt = np.random.randn(Nt, dx)\n",
    "        Yt = np.random.randn(Nt, dy)\n",
    "        Zt = np.concatenate((Xt, Yt), axis=1)  # test data\n",
    "\n",
    "        Gt = np.sign(Xt @ u)\n",
    "    \n",
    "        # Feature Map\n",
    "    \n",
    "\n",
    "        V = np.random.randn(k, d) / np.sqrt(d)\n",
    "        \n",
    "        Phi = phi(Z @ V.transpose())\n",
    "        Phi_a = phi(Za @ V.transpose())\n",
    "        Phi_t = phi(Zt @ V.transpose())\n",
    "        \n",
    "        V_inv =  np.linalg.pinv(V.transpose())\n",
    "        P = V_inv @ V.transpose()\n",
    "        # Sigma = V_inv @ V_inv.transpose()\n",
    "\n",
    "    \n",
    "        # Solution\n",
    "        # K_r = Phi_1 @ Phi_1.transpose()\n",
    "        K_inv = np.linalg.inv(Phi @ Phi.transpose())\n",
    "        pPhi = Phi.transpose() @ K_inv\n",
    "\n",
    "        # Sigma = np.linalg.inv(d * k / 2 * np.eye(d * k) + Phi_1.transpose() @ Phi_1)\n",
    "        eps = np.sqrt(d) / (100 * k) * V @ np.random.randn(d)  # np.random.multivariate_normal(np.zeros(k), Sigma)  #  * np.sqrt(d / N)\n",
    "        \n",
    "        theta = np.linalg.pinv(Phi) @ G\n",
    "        theta_1 = 0.5 * V @ Z.transpose() @ K_inv @ G + eps\n",
    "        theta_2 = P @ theta + eps\n",
    "    \n",
    "        # Scores\n",
    "    \n",
    "        score_attack = 0\n",
    "        for i in range(N):\n",
    "            if np.inner(Phi_a[i], theta) * G[i] > 0:\n",
    "                score_attack += 1\n",
    "        score_attack /= N\n",
    "    \n",
    "        score_test = 0\n",
    "        for i in range(Nt):\n",
    "            if np.inner(Phi_t[i], theta) * Gt[i] > 0:\n",
    "                score_test += 1\n",
    "        score_test /= Nt\n",
    "    \n",
    "        score_train = 0  # sanity check\n",
    "        for i in range(N):\n",
    "            if np.inner(Phi[i], theta) * G[i] > 0:\n",
    "                score_train += 1\n",
    "        score_train /= N\n",
    "    \n",
    "    \n",
    "        # Scores 1\n",
    "    \n",
    "        score_attack_1 = 0\n",
    "        for i in range(N):\n",
    "            if np.inner(Phi_a[i], theta_1) * G[i] > 0:\n",
    "                score_attack_1 += 1\n",
    "        score_attack_1 /= N\n",
    "    \n",
    "        score_test_1 = 0\n",
    "        for i in range(Nt):\n",
    "            if np.inner(Phi_t[i], theta_1) * Gt[i] > 0:\n",
    "                score_test_1 += 1\n",
    "        score_test_1 /= Nt\n",
    "    \n",
    "        score_train_1 = 0\n",
    "        for i in range(N):\n",
    "            if np.inner(Phi[i], theta_1) * G[i] > 0:\n",
    "                score_train_1 += 1\n",
    "        score_train_1 /= N\n",
    "    \n",
    "    \n",
    "        # Scores 2\n",
    "    \n",
    "        score_attack_2 = 0\n",
    "        for i in range(N):\n",
    "            if np.inner(Phi_a[i], theta_2) * G[i] > 0:\n",
    "                score_attack_2 += 1\n",
    "        score_attack_2 /= N\n",
    "    \n",
    "        score_test_2 = 0\n",
    "        for i in range(Nt):\n",
    "            if np.inner(Phi_t[i], theta_2) * Gt[i] > 0:\n",
    "                score_test_2 += 1\n",
    "        score_test_2 /= Nt\n",
    "    \n",
    "        score_train_2 = 0\n",
    "        for i in range(N):\n",
    "            if np.inner(Phi[i], theta_2) * G[i] > 0:\n",
    "                score_train_2 += 1\n",
    "        score_train_2 /= N\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba4ad23f-02f3-4b2f-9ed4-4cb5dfaf3ae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17.474462199834413"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# z_t = np.random.randn(d)\n",
    "y = np.random.randn(dy)\n",
    "z_t = np.concatenate((np.sqrt(dx) * u, y))\n",
    "phi_z = phi(V @ z_t)\n",
    "\n",
    "f = theta.transpose() @ phi_z\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1740960b-8c5e-490f-a33d-4f16e850870d",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = relu\n",
    "dev_phi = dev_relu\n",
    "\n",
    "gt = 'sign'\n",
    "\n",
    "k = 10000\n",
    "d = 100\n",
    "N = 1000\n",
    "\n",
    "# xt = np.random.randn(d)\n",
    "u = np.random.randn(d) / np.sqrt(d)\n",
    "X = np.random.randn(N, d) #  + np.outer(np.ones(N), xt) / np.sqrt(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "81cc56d7-32ef-4160-88eb-2c4493ba6ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'sign' in gt:\n",
    "    G = np.sign(X @ u)\n",
    "    xt = np.sqrt(d) * X.transpose() @ G / np.linalg.norm(X.transpose() @ G)\n",
    "    Gt = np.sign(np.inner(xt, u)) # np.sign(Xt @ u)\n",
    "elif 'linear' in gt:\n",
    "    G = X @ u\n",
    "    xt = np.sqrt(d) * X.transpose() @ G / np.linalg.norm(X.transpose() @ G)\n",
    "    Gt = np.inner(xt, u) # Xt @ u\n",
    "elif 'square' in gt:\n",
    "    G = (X @ u) ** 2\n",
    "    xt = np.sqrt(d) * X.transpose() @ G / np.linalg.norm(X.transpose() @ G)\n",
    "    Gt = (np.inner(xt, u)) ** 2 # (Xt @ u) ** 2\n",
    "\n",
    "V = np.random.randn(k, d) / np.sqrt(d)\n",
    "Phi = phi(X @ V.transpose())\n",
    "phi_t = phi(V @ xt)\n",
    "\n",
    "V_inv =  np.linalg.pinv(V.transpose())\n",
    "P = V_inv @ V.transpose()\n",
    "\n",
    "# ort_test = (np.eye(k) - P) @ Phi_t[0]\n",
    "    \n",
    "theta = np.linalg.pinv(Phi) @ G\n",
    "theta_1 = P @ theta\n",
    "\n",
    "\n",
    "test_loss = np.linalg.norm(np.inner(phi_t, theta) - Gt) ** 2\n",
    "train_loss = np.linalg.norm(Phi @ theta - G) ** 2 / N\n",
    "test_loss_1 = np.linalg.norm(np.inner(phi_t, theta_1) - Gt) ** 2\n",
    "train_loss_1 = np.linalg.norm(Phi @ theta_1 - G) ** 2 / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f9c2163d-6a46-4721-b406-420c4eacb9ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.20519857915207"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa55ada1-8eac-458f-b208-6fe9d1d93334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.60096694,  0.27174977, -0.21172676, ..., -0.48479041,\n",
       "        -0.96792269,  1.63459134],\n",
       "       [ 0.13787489,  0.77991542,  0.25285986, ..., -0.94782649,\n",
       "         0.8341446 , -0.35706429],\n",
       "       [-0.47516119,  0.61757114, -0.13649291, ...,  0.07088566,\n",
       "         1.17678482,  1.75481149],\n",
       "       ...,\n",
       "       [-0.59335394, -1.00680831,  0.42989737, ...,  2.33793473,\n",
       "         0.29115917,  0.47490082],\n",
       "       [-0.12345399,  0.32266323, -0.12814617, ...,  2.43098824,\n",
       "         1.74532975,  0.2274105 ],\n",
       "       [ 1.00606762,  0.67662096,  0.8584355 , ...,  0.44042554,\n",
       "         1.9993179 ,  0.14236148]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476744f0-57cf-4c63-addd-a0d214832db1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a66286-068f-4053-adbb-4fda0f6d2d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2330047f-1a73-4c4a-a52a-bd155ca8f3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d61b28a-e0b0-4fe9-ab9e-0e559bd4ca82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50070f03-0598-4ad6-9dab-18d2fd1e8e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a497114c-44f7-4bb0-84a4-7d27c02fa09e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d52ada8a-e953-423f-978c-1c61983175fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import argparse\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a69e79a8-cd36-417c-8823-4b4499c40755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--k')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# k = int(args.k)\n",
    "# time.sleep(k)\n",
    "\n",
    "k = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "425d0d33-46f7-4ebf-b590-04c7d507cef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = os.path.join('NN', '01_09_fullbatch_3')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=60000, shuffle=True, num_workers=1, pin_memory=True  # Full batch\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=10000, shuffle=True, num_workers=1, pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b6ba3b0c-53e8-4a98-b977-e11846b641f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleFCN(nn.Module):\n",
    "    def __init__(self, width):\n",
    "        super(SimpleFCN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, width, bias=False)  # no bias\n",
    "        self.fc2 = nn.Linear(width, 10, bias=False)  # no bias\n",
    "        # nn.init.zeros_(self.fc2.weight)  # 0 init\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_full_batch(model, train_loader, optimizer, criterion, device, sample_size=100):\n",
    "    model.train()\n",
    "    param1_grad_norms = []\n",
    "    param2_grad_norms = []\n",
    "\n",
    "    # Get the full batch of data\n",
    "    data, target = next(iter(train_loader))\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    # Calculate the gradient norms for a subset of the samples\n",
    "    for i in range(sample_size):\n",
    "        sample_grad_norm_param1 = 0\n",
    "        sample_grad_norm_param2 = 0\n",
    "        model.zero_grad()  # Reset gradients for each sample\n",
    "        output = model(data[i].unsqueeze(0))  # Forward pass for the single sample\n",
    "        loss = criterion(output, target[i].unsqueeze(0))  # Compute loss for the sample\n",
    "        loss.backward()  # Compute gradients for this sample\n",
    "    \n",
    "        for j, param in enumerate(model.parameters()):\n",
    "            if param.grad is not None:\n",
    "                if j == 0:  # First parameter (hidden layer)\n",
    "                    sample_grad_norm_param1 += param.grad.norm().item() ** 2\n",
    "                elif j == 1:  # Second parameter (output layer)\n",
    "                    sample_grad_norm_param2 += param.grad.norm().item() ** 2\n",
    "\n",
    "        param1_grad_norms.append(np.sqrt(sample_grad_norm_param1))\n",
    "        param2_grad_norms.append(np.sqrt(sample_grad_norm_param2))\n",
    "\n",
    "    # After computing the per-sample gradients, perform the actual full-batch optimization step\n",
    "    optimizer.zero_grad()  # Reset gradients before full batch forward pass\n",
    "    output = model(data)\n",
    "    loss = criterion(output, target)\n",
    "    loss.backward()  # Compute gradients for the full batch\n",
    "\n",
    "    optimizer.step()  # Perform the optimizer step after computing gradients for the full batch\n",
    "\n",
    "    avg_grad_norm_param1 = np.mean(param1_grad_norms)\n",
    "    avg_grad_norm_param2 = np.mean(param2_grad_norms)\n",
    "\n",
    "    return loss.item(), avg_grad_norm_param1, avg_grad_norm_param2\n",
    "\n",
    "\n",
    "def test(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    # test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "160af4c1-c525-4467-89da-574b704f190c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# widths = [100, 200, 500, 1000, 2000, 5000, 10000]\n",
    "# width = widths[k % 7]\n",
    "# k = k // 7\n",
    "\n",
    "width = 1000\n",
    "\n",
    "model = SimpleFCN(width)\n",
    "\n",
    "# Clrs = [0.001, 0.01, 0.1, 1, 10]\n",
    "# Clr = Clrs[k % 5]\n",
    "# k = k // 5\n",
    "# lr = 1000 * Clr / width\n",
    "\n",
    "lr = 0.1\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_accuracies = []\n",
    "grad_norms_param1 = []\n",
    "grad_norms_param2 = []\n",
    "\n",
    "T = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce0bcf61-fe7e-4af7-9ec5-b99609cbc32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train Loss: 2.284513, Test Loss: 1.985296, Accuracy: 55.13%\n",
      "Avg Grad Norm Param1: 10.885303, Avg Grad Norm Param2: 12.444546\n",
      "Epoch 2\n",
      "Train Loss: 1.990868, Test Loss: 1.744897, Accuracy: 68.12%\n",
      "Avg Grad Norm Param1: 10.183039, Avg Grad Norm Param2: 11.547263\n",
      "Epoch 3\n",
      "Train Loss: 1.755890, Test Loss: 1.538762, Accuracy: 73.27%\n",
      "Avg Grad Norm Param1: 10.114790, Avg Grad Norm Param2: 11.408100\n",
      "Epoch 4\n",
      "Train Loss: 1.554209, Test Loss: 1.363927, Accuracy: 76.77%\n",
      "Avg Grad Norm Param1: 9.439765, Avg Grad Norm Param2: 10.746394\n",
      "Epoch 5\n",
      "Train Loss: 1.382727, Test Loss: 1.218139, Accuracy: 79.04%\n",
      "Avg Grad Norm Param1: 9.116814, Avg Grad Norm Param2: 10.384436\n",
      "Epoch 6\n",
      "Train Loss: 1.239348, Test Loss: 1.098219, Accuracy: 80.57%\n",
      "Avg Grad Norm Param1: 8.621726, Avg Grad Norm Param2: 9.720040\n",
      "Epoch 7\n",
      "Train Loss: 1.121013, Test Loss: 1.000088, Accuracy: 81.63%\n",
      "Avg Grad Norm Param1: 8.707281, Avg Grad Norm Param2: 9.888912\n",
      "Epoch 8\n",
      "Train Loss: 1.023869, Test Loss: 0.919686, Accuracy: 82.57%\n",
      "Avg Grad Norm Param1: 8.160784, Avg Grad Norm Param2: 9.260941\n",
      "Epoch 9\n",
      "Train Loss: 0.944004, Test Loss: 0.853392, Accuracy: 83.31%\n",
      "Avg Grad Norm Param1: 7.821206, Avg Grad Norm Param2: 8.762348\n",
      "Epoch 10\n",
      "Train Loss: 0.877943, Test Loss: 0.798225, Accuracy: 83.89%\n",
      "Avg Grad Norm Param1: 7.855391, Avg Grad Norm Param2: 8.895272\n",
      "Epoch 11\n",
      "Train Loss: 0.822815, Test Loss: 0.751846, Accuracy: 84.64%\n",
      "Avg Grad Norm Param1: 7.379218, Avg Grad Norm Param2: 8.298932\n",
      "Epoch 12\n",
      "Train Loss: 0.776347, Test Loss: 0.712447, Accuracy: 85.13%\n",
      "Avg Grad Norm Param1: 7.760192, Avg Grad Norm Param2: 8.747116\n",
      "Epoch 13\n",
      "Train Loss: 0.736773, Test Loss: 0.678636, Accuracy: 85.62%\n",
      "Avg Grad Norm Param1: 7.176372, Avg Grad Norm Param2: 7.994911\n",
      "Epoch 14\n",
      "Train Loss: 0.702729, Test Loss: 0.649345, Accuracy: 85.95%\n",
      "Avg Grad Norm Param1: 6.319916, Avg Grad Norm Param2: 7.127066\n",
      "Epoch 15\n",
      "Train Loss: 0.673169, Test Loss: 0.623742, Accuracy: 86.21%\n",
      "Avg Grad Norm Param1: 6.317748, Avg Grad Norm Param2: 7.035504\n",
      "Epoch 16\n",
      "Train Loss: 0.647273, Test Loss: 0.601182, Accuracy: 86.62%\n",
      "Avg Grad Norm Param1: 6.872006, Avg Grad Norm Param2: 7.520608\n",
      "Epoch 17\n",
      "Train Loss: 0.624412, Test Loss: 0.581156, Accuracy: 86.90%\n",
      "Avg Grad Norm Param1: 6.199687, Avg Grad Norm Param2: 6.889755\n",
      "Epoch 18\n",
      "Train Loss: 0.604085, Test Loss: 0.563265, Accuracy: 87.17%\n",
      "Avg Grad Norm Param1: 5.643668, Avg Grad Norm Param2: 6.240262\n",
      "Epoch 19\n",
      "Train Loss: 0.585893, Test Loss: 0.547185, Accuracy: 87.34%\n",
      "Avg Grad Norm Param1: 6.648116, Avg Grad Norm Param2: 7.144110\n",
      "Epoch 20\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, T \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m     train_loss, avg_grad_norm_param1, avg_grad_norm_param2 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_full_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m test(model, test_loader, criterion, device)\n\u001b[1;32m      6\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[6], line 21\u001b[0m, in \u001b[0;36mtrain_full_batch\u001b[0;34m(model, train_loader, optimizer, criterion, device, sample_size)\u001b[0m\n\u001b[1;32m     18\u001b[0m param2_grad_norms \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Get the full batch of data\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m data, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(train_loader))\n\u001b[1;32m     22\u001b[0m data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device), target\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Calculate the gradient norms for a subset of the samples\u001b[39;00m\n",
      "File \u001b[0;32m~/DP-KAN/venv_kan/lib/python3.11/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/DP-KAN/venv_kan/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/DP-KAN/venv_kan/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1285\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1283\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_thread\u001b[38;5;241m.\u001b[39mis_alive():\n\u001b[0;32m-> 1285\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1287\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/DP-KAN/venv_kan/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1133\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1136\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.11/queue.py:180\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m remaining \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m    179\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m--> 180\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnot_empty\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    181\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get()\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnot_full\u001b[38;5;241m.\u001b[39mnotify()\n",
      "File \u001b[0;32m/usr/lib/python3.11/threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, T + 1):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    train_loss, avg_grad_norm_param1, avg_grad_norm_param2 = train_full_batch(model, train_loader, optimizer, criterion, device)\n",
    "    test_loss, test_accuracy = test(model, test_loader, criterion, device)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    grad_norms_param1.append(avg_grad_norm_param1)\n",
    "    grad_norms_param2.append(avg_grad_norm_param2)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.6f}, Test Loss: {test_loss:.6f}, Accuracy: {test_accuracy:.2f}%\")\n",
    "    print(f\"Avg Grad Norm Param1: {avg_grad_norm_param1:.6f}, Avg Grad Norm Param2: {avg_grad_norm_param2:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eea0e82-faae-47b7-aed5-3de15a214d76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd10aab6-9ceb-44a1-9ca2-27f8e945af16",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9ddf6d9-ae74-4737-9aec-b15fa24de905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 1, 0, 4])\n",
      "tensor([[-0.0074,  0.0664, -0.0074, -0.0074, -0.0074, -0.0074, -0.0074, -0.0074,\n",
      "         -0.0074, -0.0074],\n",
      "        [-0.0072,  0.0652, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072, -0.0072,\n",
      "         -0.0072, -0.0072],\n",
      "        [-0.0100,  0.0899, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100, -0.0100,\n",
      "         -0.0100, -0.0100],\n",
      "        [-0.0085,  0.0769, -0.0085, -0.0085, -0.0085, -0.0085, -0.0085, -0.0085,\n",
      "         -0.0085, -0.0085],\n",
      "        [-0.0043,  0.0385, -0.0043, -0.0043, -0.0043, -0.0043, -0.0043, -0.0043,\n",
      "         -0.0043, -0.0043]])\n",
      "2.2997887134552\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    data, target = next(iter(test_loader))\n",
    "    data, target = data.to(device), target.to(device)\n",
    "\n",
    "    print(target[:5])\n",
    "    \n",
    "    output = model(data)\n",
    "\n",
    "    print(output[:5])\n",
    "    \n",
    "    test_loss += criterion(output, target).item()\n",
    "\n",
    "    print(criterion(output, target).item())\n",
    "    \n",
    "    pred = output.argmax(dim=1, keepdim=True)\n",
    "    correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100. * correct / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f8072-1d2c-4499-afae-17ddbf978781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0301ad02-8499-4964-8a6a-48f8c9fefd0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733ade02-79d3-4b65-b92c-6d40d5f1b11c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (162238052.py, line 155)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 155\u001b[0;36m\u001b[0m\n\u001b[0;31m    'grad_norms_param1': grad_norms_param1,\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Save results to JSON file\n",
    "results = {\n",
    "    'width': width,\n",
    "    'Clr': Clr,\n",
    "    'train_losses': train_losses,\n",
    "    'test_losses': test_losses,\n",
    "    'test_accuracies': test_accuracies\n",
    "    'grad_norms_param1': grad_norms_param1,\n",
    "    'grad_norms_param2': grad_norms_param2\n",
    "}\n",
    "\n",
    "with open(os.path.join(save_dir, f'results_{args.k}.json'), 'w') as f:\n",
    "    json.dump(results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3bcb2f-6135-4f02-8199-b903f6da6325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dccbcb-cc6f-45d7-b2ec-00b9c86911e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970ac222-606a-4479-8f43-e9b5e1fcefaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43898075-675c-4702-a5a0-8675dcbe66f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_clipped_gradients(sample_data, sample_target, model, criterion, clip_values, device):\n",
    "    \n",
    "    sample_data, sample_target = sample_data.to(device), sample_target.to(device)  # no batch dimension\n",
    "    \n",
    "    output = model(sample_data.unsqueeze(0))\n",
    "    loss = criterion(output, sample_target.unsqueeze(0))\n",
    "    \n",
    "    total_grads = [torch.zeros_like(param) for param in model.parameters()]\n",
    "\n",
    "    model.zero_grad()\n",
    "    grads = torch.autograd.grad(loss, model.parameters(), retain_graph=True)  # why retain graph?\n",
    "\n",
    "    for j, grad in enumerate(grads):\n",
    "        grad_norm = grad.norm()\n",
    "        clip_value = clip_values[j]\n",
    "        if grad_norm > clip_value:\n",
    "            grad = grad * (clip_value / grad_norm)\n",
    "        grads[j] = grad\n",
    "    \n",
    "    return grads\n",
    "\n",
    "\n",
    "def train_dp_batch(model, train_loader, lr, n, criterion, device, clip_values, sigma):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for data, target in train_loader:  # batch-size > 1\n",
    "        \n",
    "        batch_grads = compute_clipped_gradients(data, target, model, criterion, clip_values, device)\n",
    "        # seems wrong, more dimensions only in first 2 args\n",
    "\n",
    "        mean_grad = # compute mean over batch dimension\n",
    "\n",
    "        for i, param in enumerate(model.parameters()):\n",
    "            noise = np.sqrt(lr) * (2 * clip_values[i] / n) * sigma * torch.normal(0, 1, size=param.grad.size()).to(device)\n",
    "            param.data = param.data - lr * mean_grad[i] + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b56a4cbf-eaa2-442e-bf68-c7ea6694a5d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jax in /mnt/nfs/clustersw/Debian/bookworm/jupyterhub/1.0/lib/python3.11/site-packages (0.4.30)\n",
      "Requirement already satisfied: jaxlib<=0.4.30,>=0.4.27 in /mnt/nfs/clustersw/Debian/bookworm/jupyterhub/1.0/lib/python3.11/site-packages (from jax) (0.4.30)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /mnt/nfs/clustersw/Debian/bookworm/jupyterhub/1.0/lib/python3.11/site-packages (from jax) (0.3.2)\n",
      "Requirement already satisfied: numpy>=1.22 in /mnt/nfs/clustersw/Debian/bookworm/jupyterhub/1.0/lib/python3.11/site-packages (from jax) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum in /mnt/nfs/clustersw/Debian/bookworm/jupyterhub/1.0/lib/python3.11/site-packages (from jax) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in /mnt/nfs/clustersw/Debian/bookworm/jupyterhub/1.0/lib/python3.11/site-packages (from jax) (1.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install jax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba77d6d-e47d-44fc-9b6b-4a7219570c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "Training with T=100\n",
      "------------------------------------\n",
      "Epoch 10, Test Accuracy: 69.80%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 136\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_x, batch_y \u001b[38;5;129;01min\u001b[39;00m create_batches(train_images, train_labels, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[1;32m    135\u001b[0m     key, subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[0;32m--> 136\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Test accuracy every 10 epochs\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "from jax import grad, jit, vmap\n",
    "import numpy as np\n",
    "import os\n",
    "import argparse\n",
    "import time\n",
    "import json\n",
    "from functools import partial\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# # Argument parsing\n",
    "# parser = argparse.ArgumentParser()\n",
    "# parser.add_argument('--k')\n",
    "# args = parser.parse_args()\n",
    "\n",
    "# k = int(args.k)\n",
    "# time.sleep(k)\n",
    "\n",
    "\n",
    "k = 0\n",
    "\n",
    "save_dir = os.path.join('NN', '04_09_grid_jax')\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "# Load MNIST data\n",
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "train_images = train_images.reshape(-1, 28 * 28) / 255.0\n",
    "test_images = test_images.reshape(-1, 28 * 28) / 255.0\n",
    "\n",
    "# Create batches\n",
    "def create_batches(images, labels, batch_size):\n",
    "    n_batches = len(images) // batch_size\n",
    "    for i in range(n_batches):\n",
    "        yield images[i * batch_size:(i + 1) * batch_size], labels[i * batch_size:(i + 1) * batch_size]\n",
    "\n",
    "# Initialize parameters for the simple fully connected network\n",
    "def init_params(width, key):\n",
    "    key1, key2 = jax.random.split(key)\n",
    "    w1 = jax.random.normal(key1, (28 * 28, width))\n",
    "    w2 = jax.random.normal(key2, (width, 10))\n",
    "    return w1, w2\n",
    "\n",
    "# Forward pass\n",
    "def forward(params, x):\n",
    "    w1, w2 = params\n",
    "    h = jax.nn.relu(jnp.dot(x, w1))\n",
    "    return jnp.dot(h, w2)\n",
    "\n",
    "# Loss function (cross-entropy)\n",
    "def loss_fn(params, x, y):\n",
    "    logits = forward(params, x)\n",
    "    one_hot = jax.nn.one_hot(y, 10)\n",
    "    return jnp.mean(optax.softmax_cross_entropy(logits, one_hot))\n",
    "\n",
    "# Gradient clipping\n",
    "def clip_grads(grads, clip_values):\n",
    "    clipped_grads = []\n",
    "    for grad, clip_value in zip(grads, clip_values):\n",
    "        norm = jnp.linalg.norm(grad)\n",
    "        factor = jnp.minimum(1.0, clip_value / (norm + 1e-6))\n",
    "        clipped_grads.append(grad * factor)\n",
    "    return clipped_grads\n",
    "\n",
    "# Training step\n",
    "@jit\n",
    "def train_step(params, x, y, lr, clip_values, sigma, key):\n",
    "    grads = grad(loss_fn)(params, x, y)\n",
    "    grads = clip_grads(grads, clip_values)\n",
    "    noise = [(jax.random.normal(key, g.shape) * sigma) for g in grads]\n",
    "    updated_params = [(w - lr * (g + n)) for w, g, n in zip(params, grads, noise)]\n",
    "    return updated_params\n",
    "\n",
    "# Test function\n",
    "@jit\n",
    "def test_fn(params, x, y):\n",
    "    predictions = forward(params, x)\n",
    "    predicted_labels = jnp.argmax(predictions, axis=1)\n",
    "    accuracy = jnp.mean(predicted_labels == y)\n",
    "    return accuracy\n",
    "\n",
    "# # Main training loop\n",
    "# widths = [100, 1000, 10000, 100000]\n",
    "# width = widths[k % 4]\n",
    "# k = k // 4\n",
    "\n",
    "width = 1000\n",
    "\n",
    "lr = 0.1\n",
    "n = 60000\n",
    "delta = 1 / n\n",
    "# epsilons = [1, 4]\n",
    "# eps = epsilons[k % 2]\n",
    "# k = k // 2\n",
    "eps = 4\n",
    "\n",
    "\n",
    "# clip_values_1 = np.logspace(0, 1, 5)\n",
    "# clip_value_1 = clip_values_1[k % 5]\n",
    "# k = k // 5\n",
    "\n",
    "clip_value_1 = 1\n",
    "\n",
    "# C2s = np.logspace(-1, 0, 5)\n",
    "# C2 = C2s[k % 5]\n",
    "# k = k // 5\n",
    "\n",
    "C2 = 0.5\n",
    "\n",
    "clip_value_2 = C2 * np.sqrt(width)\n",
    "clip_values = [clip_value_1, clip_value_2]\n",
    "\n",
    "# Tps = np.logspace(1, 3, 5)\n",
    "\n",
    "# for Tp in Tps:\n",
    "\n",
    "\n",
    "# T = int(Tp * np.sqrt(10000 / width))\n",
    "\n",
    "T = 100\n",
    "\n",
    "print(\"------------------------------------\")\n",
    "print(f\"Training with T={T}\")\n",
    "print(\"------------------------------------\")\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = init_params(width, key)\n",
    "\n",
    "sigma = np.sqrt(2) * np.sqrt(lr * T) * np.sqrt(8 * jnp.log(1 / delta)) / eps\n",
    "\n",
    "for epoch in range(1, T + 1):\n",
    "    for batch_x, batch_y in create_batches(train_images, train_labels, batch_size=128):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        params = train_step(params, batch_x, batch_y, lr, clip_values, sigma, subkey)\n",
    "\n",
    "    # Test accuracy every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        accuracy = test_fn(params, test_images, test_labels)\n",
    "        print(f\"Epoch {epoch}, Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "with open(os.path.join(save_dir, 'parameters_' + args.k + '.txt'), 'a') as f:\n",
    "    f.write(f\"{n}\\t{width}\\t{accuracy:.4f}\\t{eps}\\t{T}\\t{clip_value_1:.2f}\\t{clip_value_2:.2f}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1519bde-5646-447a-a1dc-de871857061f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e16e9d1-d09c-408c-ad47-482ef13f6f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 09:04:57.447987: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-09-09 09:04:58.467117: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-09-09 09:04:58.702488: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-09 09:05:38.206569: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Train loss = 2.1503, Test accuracy = 0.3482\n",
      "Average gradient norms: (Array(2., dtype=float32), Array(5., dtype=float32))\n",
      "Epoch 1: Train loss = 1.4947, Test accuracy = 0.5602\n",
      "Average gradient norms: (Array(1.8493034, dtype=float32), Array(4.4057155, dtype=float32))\n",
      "Epoch 2: Train loss = 1.2346, Test accuracy = 0.6339\n",
      "Average gradient norms: (Array(1.9148229, dtype=float32), Array(4.600794, dtype=float32))\n",
      "Epoch 3: Train loss = 1.0162, Test accuracy = 0.7323\n",
      "Average gradient norms: (Array(1.516403, dtype=float32), Array(3.4498107, dtype=float32))\n",
      "Epoch 4: Train loss = 0.9112, Test accuracy = 0.7585\n",
      "Average gradient norms: (Array(1.6075107, dtype=float32), Array(3.6223037, dtype=float32))\n",
      "Final test accuracy: 0.7585\n",
      "Training losses: [Array(2.1503034, dtype=float32), Array(1.4947168, dtype=float32), Array(1.234557, dtype=float32), Array(1.0161875, dtype=float32), Array(0.9112235, dtype=float32)]\n",
      "Test accuracies: [Array(0.3482, dtype=float32), Array(0.5602, dtype=float32), Array(0.6339, dtype=float32), Array(0.7323, dtype=float32), Array(0.7585, dtype=float32)]\n",
      "Average gradient norms: [(Array(2., dtype=float32), Array(5., dtype=float32)), (Array(1.8493034, dtype=float32), Array(4.4057155, dtype=float32)), (Array(1.9148229, dtype=float32), Array(4.600794, dtype=float32)), (Array(1.516403, dtype=float32), Array(3.4498107, dtype=float32)), (Array(1.6075107, dtype=float32), Array(3.6223037, dtype=float32))]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap, nn\n",
    "import optax\n",
    "import tensorflow_datasets as tfds\n",
    "import jax.random as jrandom\n",
    "import jax\n",
    "from jax.tree_util import tree_map, tree_leaves\n",
    "\n",
    "\n",
    "def load_mnist():\n",
    "    try:\n",
    "        (train_data, train_labels), (test_data, test_labels) = tfds.load('mnist', split=['train', 'test'], batch_size=-1, as_supervised=True)\n",
    "        train_data = jnp.array(train_data, dtype=jnp.float32) / 255.0\n",
    "        test_data = jnp.array(test_data, dtype=jnp.float32) / 255.0\n",
    "        \n",
    "        # Flatten images\n",
    "        train_data = train_data.reshape(train_data.shape[0], -1)\n",
    "        test_data = test_data.reshape(test_data.shape[0], -1)\n",
    "        \n",
    "        # Normalize\n",
    "        mean = jnp.mean(train_data, axis=0)\n",
    "        std = jnp.std(train_data, axis=0)\n",
    "        train_data = (train_data - mean) / (std + 1e-8)\n",
    "        test_data = (test_data - mean) / (std + 1e-8)\n",
    "        \n",
    "        train_labels = jnp.array(train_labels)\n",
    "        test_labels = jnp.array(test_labels)\n",
    "        \n",
    "        return train_data, train_labels, test_data, test_labels\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading MNIST dataset: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "\n",
    "def initialize_params(input_dim, hidden_dim, output_dim, key):\n",
    "    key1, key2 = jrandom.split(key)\n",
    "    \n",
    "    # He initialization\n",
    "    stddev1 = jnp.sqrt(2.0 / input_dim)\n",
    "    stddev2 = jnp.sqrt(2.0 / hidden_dim)\n",
    "\n",
    "    V_1 = jrandom.normal(key1, (hidden_dim, input_dim)) * stddev1\n",
    "    V_2 = jrandom.normal(key2, (output_dim, hidden_dim)) * stddev2\n",
    "\n",
    "    return (V_1, V_2)\n",
    "\n",
    "\n",
    "def predict(params, x):\n",
    "    V_1, V_2 = params\n",
    "    pre_act = jnp.matmul(x, V_1.T)\n",
    "    act = nn.relu(pre_act)\n",
    "    out = jnp.matmul(act, V_2.T)\n",
    "    return out\n",
    "\n",
    "\n",
    "def loss(params, x, target):\n",
    "    pred = predict(params, x)\n",
    "    return optax.softmax_cross_entropy_with_integer_labels(pred, target)\n",
    "\n",
    "\n",
    "def clip_gradient(g, clip_norm):\n",
    "    grad_norm = jnp.linalg.norm(g)\n",
    "    return jnp.where(grad_norm <= clip_norm, g, g * (clip_norm / grad_norm))\n",
    "\n",
    "\n",
    "def evaluate(params, data, labels):\n",
    "    predictions = predict(params, data)\n",
    "    predicted_labels = jnp.argmax(predictions, axis=1)\n",
    "    accuracy = jnp.mean(predicted_labels == labels)\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "@jit\n",
    "def accumulate_grads(accumulated_grads, batch_grads):\n",
    "    return tree_map(lambda acc, batch: acc + batch, accumulated_grads, batch_grads)\n",
    "\n",
    "\n",
    "def compute_batch_clipped_grads(params, x, y, clip_norms, batch_idx):\n",
    "    def single_sample_loss(params, x, y):\n",
    "        pred = predict(params, x)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(pred, y)\n",
    "\n",
    "    batch_grad_fn = vmap(grad(single_sample_loss), in_axes=(None, 0, 0))\n",
    "    per_sample_grads = batch_grad_fn(params, x, y)\n",
    "\n",
    "    # Clip gradients layer-wise\n",
    "    per_sample_clipped_grads = tree_map(lambda g, c: vmap(clip_gradient, in_axes=(0, None))(g, c),\n",
    "                                        per_sample_grads, clip_norms)\n",
    "\n",
    "\n",
    "    # Debugging: Print information for the first few samples\n",
    "    # def debug_gradients(grads, clipped_grads):\n",
    "    #     for i in range(min(10, x.shape[0])):\n",
    "    #         print(f\"Sample {i}:\")\n",
    "    #         for j, (g, cg) in enumerate(zip(tree_leaves(grads), tree_leaves(clipped_grads))):\n",
    "    #             print(f\"  Param {j}:\")\n",
    "    #             print(f\"    Original norm: {jnp.linalg.norm(g[i]):.4f}\")\n",
    "    #             print(f\"    Clipped norm: {jnp.linalg.norm(cg[i]):.4f}\")\n",
    "    #             print(f\"    Clip norm: {clip_norms[j]:.4f}\")\n",
    "    #         print()\n",
    "\n",
    "    # print(\"Debugging gradient information:\")\n",
    "    # debug_gradients(per_sample_grads, per_sample_clipped_grads)\n",
    "    \n",
    "    # Sum clipped gradients for this batch\n",
    "    batch_sum_clipped_grads = tree_map(lambda g: jnp.sum(g, axis=0), per_sample_clipped_grads)\n",
    "\n",
    "    if batch_idx == 0:\n",
    "        avg_grad_norms = tree_map(lambda g: jnp.mean(jnp.linalg.norm(g, axis=(-2, -1))), per_sample_clipped_grads)\n",
    "        return batch_sum_clipped_grads, avg_grad_norms\n",
    "    else:\n",
    "        return batch_sum_clipped_grads, None\n",
    "\n",
    "\n",
    "def training_step_full(params, train_data, train_labels, learning_rate, noise_multiplier, clip_norms, key, batch_size):\n",
    "    num_samples = train_data.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    accumulated_grads = tree_map(lambda p: jnp.zeros_like(p), params)\n",
    "    first_batch_avg_grad_norms = None\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        batch_data = train_data[start_idx:end_idx]\n",
    "        batch_labels = train_labels[start_idx:end_idx]\n",
    "        batch_grads, avg_grad_norms = compute_batch_clipped_grads(params, batch_data, batch_labels, clip_norms, i)\n",
    "        accumulated_grads = accumulate_grads(accumulated_grads, batch_grads)\n",
    "        \n",
    "        if i == 0:\n",
    "            first_batch_avg_grad_norms = avg_grad_norms\n",
    "    \n",
    "    avg_grads = tree_map(lambda g: g / num_samples, accumulated_grads)\n",
    "    keys = tuple(jrandom.split(key, len(tree_leaves(avg_grads))))\n",
    "    noises = tree_map(\n",
    "        lambda c, k, g: noise_multiplier * c * jrandom.normal(k, g.shape),\n",
    "        clip_norms, keys, avg_grads\n",
    "    )\n",
    "    new_params = tree_map(lambda p, g, n: p - learning_rate * g + n, params, avg_grads, noises)\n",
    "    return new_params, first_batch_avg_grad_norms\n",
    "\n",
    "        \n",
    "def training_step_no_noise(params, train_data, train_labels, learning_rate, clip_norms, batch_size):\n",
    "    num_samples = train_data.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    accumulated_grads = tree_map(lambda p: jnp.zeros_like(p), params)\n",
    "    first_batch_avg_grad_norms = None\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        batch_data = train_data[start_idx:end_idx]\n",
    "        batch_labels = train_labels[start_idx:end_idx]\n",
    "        batch_grads, avg_grad_norms = compute_batch_clipped_grads(params, batch_data, batch_labels, clip_norms, i)\n",
    "        accumulated_grads = accumulate_grads(accumulated_grads, batch_grads)\n",
    "        \n",
    "        if i == 0:\n",
    "            first_batch_avg_grad_norms = avg_grad_norms\n",
    "    \n",
    "    avg_grads = tree_map(lambda g: g / num_samples, accumulated_grads)\n",
    "    new_params = tree_map(lambda p, g: p - learning_rate * g, params, avg_grads)\n",
    "    return new_params, first_batch_avg_grad_norms\n",
    "\n",
    "\n",
    "\n",
    "def training_step_no_clip(params, train_data, train_labels, learning_rate, batch_size):\n",
    "    def compute_batch_grads(params, x, y, batch_idx):\n",
    "        def single_sample_loss(params, x, y):\n",
    "            pred = predict(params, x)\n",
    "            return optax.softmax_cross_entropy_with_integer_labels(pred, y)\n",
    "        batch_grad_fn = vmap(grad(single_sample_loss), in_axes=(None, 0, 0))\n",
    "        batch_grads = batch_grad_fn(params, x, y)\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            avg_grad_norms = tree_map(lambda g: jnp.mean(jnp.linalg.norm(g, axis=(-2, -1))), batch_grads)\n",
    "            return jnp.sum(batch_grads, axis=0), avg_grad_norms\n",
    "        else:\n",
    "            return jnp.sum(batch_grads, axis=0), None\n",
    "    \n",
    "    num_samples = train_data.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    accumulated_grads = tree_map(lambda p: jnp.zeros_like(p), params)\n",
    "    first_batch_avg_grad_norms = None\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        batch_data = train_data[start_idx:end_idx]\n",
    "        batch_labels = train_labels[start_idx:end_idx]\n",
    "        batch_grads, avg_grad_norms = compute_batch_grads(params, batch_data, batch_labels, i)\n",
    "        accumulated_grads = accumulate_grads(accumulated_grads, batch_grads)\n",
    "        \n",
    "        if i == 0:\n",
    "            first_batch_avg_grad_norms = avg_grad_norms\n",
    "    \n",
    "    avg_grads = tree_map(lambda g: g / num_samples, accumulated_grads)\n",
    "    new_params = tree_map(lambda p, g: p - learning_rate * g, params, avg_grads)\n",
    "    return new_params, first_batch_avg_grad_norms\n",
    "\n",
    "\n",
    "\n",
    "def training_step_no_clip_with_noise(params, train_data, train_labels, learning_rate, noise_multiplier, key, batch_size):\n",
    "    def compute_batch_grads(params, x, y, batch_idx):\n",
    "        def single_sample_loss(params, x, y):\n",
    "            pred = predict(params, x)\n",
    "            return optax.softmax_cross_entropy_with_integer_labels(pred, y)\n",
    "        batch_grad_fn = vmap(grad(single_sample_loss), in_axes=(None, 0, 0))\n",
    "        batch_grads = batch_grad_fn(params, x, y)\n",
    "        \n",
    "        if batch_idx == 0:\n",
    "            avg_grad_norms = tree_map(lambda g: jnp.mean(jnp.linalg.norm(g, axis=(-2, -1))), batch_grads)\n",
    "            return jnp.sum(batch_grads, axis=0), avg_grad_norms\n",
    "        else:\n",
    "            return jnp.sum(batch_grads, axis=0), None\n",
    "    \n",
    "    num_samples = train_data.shape[0]\n",
    "    num_batches = num_samples // batch_size\n",
    "    accumulated_grads = tree_map(lambda p: jnp.zeros_like(p), params)\n",
    "    first_batch_avg_grad_norms = None\n",
    "    \n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = (i + 1) * batch_size\n",
    "        batch_data = train_data[start_idx:end_idx]\n",
    "        batch_labels = train_labels[start_idx:end_idx]\n",
    "        batch_grads, avg_grad_norms = compute_batch_grads(params, batch_data, batch_labels, i)\n",
    "        accumulated_grads = accumulate_grads(accumulated_grads, batch_grads)\n",
    "        \n",
    "        if i == 0:\n",
    "            first_batch_avg_grad_norms = avg_grad_norms\n",
    "    \n",
    "    avg_grads = tree_map(lambda g: g / num_samples, accumulated_grads)\n",
    "    keys = tuple(jrandom.split(key, len(tree_leaves(avg_grads))))\n",
    "    noises = tree_map(\n",
    "        lambda k, g: noise_multiplier * jrandom.normal(k, g.shape),\n",
    "        keys, avg_grads\n",
    "    )\n",
    "    new_params = tree_map(lambda p, g, n: p - learning_rate * g + n, params, avg_grads, noises)\n",
    "    return new_params, first_batch_avg_grad_norms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(params, learning_rate, train_data, train_labels, test_data, test_labels, num_epochs, batch_size, noise_multiplier, clip_norms, print_every, training_mode='full'):\n",
    "    key = jrandom.PRNGKey(0)\n",
    "    metrics = {'train_loss': [], 'test_acc': [], 'grad_norms': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        key, subkey = jrandom.split(key)\n",
    "        \n",
    "        if training_mode == 'full':\n",
    "            params, avg_grad_norms = training_step_full(params, train_data, train_labels, learning_rate, noise_multiplier, clip_norms, subkey, batch_size)\n",
    "        elif training_mode == 'no_noise':\n",
    "            params, avg_grad_norms = training_step_no_noise(params, train_data, train_labels, learning_rate, clip_norms, batch_size)\n",
    "        elif training_mode == 'no_clip':\n",
    "            params, avg_grad_norms = training_step_no_clip(params, train_data, train_labels, learning_rate, batch_size)\n",
    "        elif training_mode == 'no_clip_with_noise':\n",
    "            params, avg_grad_norms = training_step_no_clip_with_noise(params, train_data, train_labels, learning_rate, noise_multiplier, subkey, batch_size)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid training mode\")\n",
    "\n",
    "        train_loss = jnp.mean(loss(params, train_data, train_labels))\n",
    "        test_acc = evaluate(params, test_data, test_labels)\n",
    "        \n",
    "        metrics['train_loss'].append(train_loss)\n",
    "        metrics['test_acc'].append(test_acc)\n",
    "        metrics['grad_norms'].append(avg_grad_norms)\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print(f\"Epoch {epoch}: Train loss = {train_loss:.4f}, Test accuracy = {test_acc:.4f}\")\n",
    "            print(f\"Average gradient norms: {avg_grad_norms}\")\n",
    "\n",
    "    return params, metrics\n",
    "\n",
    "\n",
    "\n",
    "train_data, train_labels, test_data, test_labels = load_mnist()\n",
    "\n",
    "# Initialize parameters\n",
    "input_dim = train_data.shape[1]\n",
    "hidden_dim = 1000\n",
    "output_dim = 10\n",
    "key = jrandom.PRNGKey(0)\n",
    "# print(key)\n",
    "params = initialize_params(input_dim, hidden_dim, output_dim, key)\n",
    "\n",
    "# Training hyperparameters\n",
    "lr = 1\n",
    "batch_size = 100\n",
    "num_samples = 60000\n",
    "T = 5\n",
    "clip_norm_v1 = 2\n",
    "clip_norm_v2 = 5\n",
    "print_every = 1\n",
    "num_epochs = T\n",
    "\n",
    "eps = 4\n",
    "delta = 1 / num_samples\n",
    "\n",
    "sigma = jnp.sqrt(2) * jnp.sqrt(lr * T) * jnp.sqrt(8 * jnp.log(1 / delta)) / eps\n",
    "noise_multiplier = jnp.sqrt(lr) * (2 * 1 / num_samples) * sigma  # The 1 is the clipping constant inside the function \n",
    "\n",
    "\n",
    "# Train the model\n",
    "final_params, metrics = train(params, lr, train_data, train_labels, test_data, test_labels,\n",
    "              num_epochs, batch_size, noise_multiplier, (clip_norm_v1, clip_norm_v2), print_every, training_mode='full')\n",
    "\n",
    "# Evaluate final model\n",
    "final_accuracy = evaluate(final_params, test_data, test_labels)\n",
    "print(f\"Final test accuracy: {final_accuracy:.4f}\")\n",
    "\n",
    "# You can now access the metrics dictionary for plotting or further analysis\n",
    "print(\"Training losses:\", metrics['train_loss'])\n",
    "print(\"Test accuracies:\", metrics['test_acc'])\n",
    "print(\"Average gradient norms:\", metrics['grad_norms'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf1cac0-7716-4c11-aae2-eb615e40c538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4226dc44-572b-45e2-8a8d-2b4212ac57a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    try:\n",
    "        from sklearn.datasets import fetch_openml\n",
    "        from sklearn.model_selection import train_test_split\n",
    "        \n",
    "        # Load MNIST from OpenML\n",
    "        X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)\n",
    "        \n",
    "        # Convert to float32 and scale to [0, 1]\n",
    "        X = X.astype(np.float32) / 255.0\n",
    "        \n",
    "        # Split into train and test sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=10000, random_state=42)\n",
    "        \n",
    "        # Convert to JAX arrays\n",
    "        train_data = jnp.array(X_train)\n",
    "        test_data = jnp.array(X_test)\n",
    "        train_labels = jnp.array(y_train.astype(int))\n",
    "        test_labels = jnp.array(y_test.astype(int))\n",
    "        \n",
    "        # Normalize\n",
    "        mean = jnp.mean(train_data, axis=0)\n",
    "        std = jnp.std(train_data, axis=0)\n",
    "        train_data = (train_data - mean) / (std + 1e-8)\n",
    "        test_data = (test_data - mean) / (std + 1e-8)\n",
    "        \n",
    "        return train_data, train_labels, test_data, test_labels\n",
    "    \n",
    "    except ImportError:\n",
    "        print(\"Scikit-learn not found!\")\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "097d6dcd-89db-4f3e-b5a2-8817b6b34ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn not found!\n"
     ]
    }
   ],
   "source": [
    "a, _, _, _ = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "576c1be9-b641-4154-8425-cba2dab2a7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-09 10:50:08.425500: W external/xla/xla/service/gpu/nvptx_compiler.cc:836] The NVIDIA driver's CUDA version is 12.2 which is older than the PTX compiler version (12.6.68). Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784) (60000,) (10000, 784) (10000,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "from PIL import Image\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "def load_mnist_local(data_dir='./data'):\n",
    "    # Load the MNIST dataset from local files\n",
    "    mnist_train = MNIST(root=data_dir, train=True, download=False)\n",
    "    mnist_test = MNIST(root=data_dir, train=False, download=False)\n",
    "\n",
    "    # Convert images and labels to NumPy arrays\n",
    "    X_train = np.array([np.array(image) for image in mnist_train.data], dtype=np.float32)\n",
    "    y_train = np.array(mnist_train.targets, dtype=int)\n",
    "    X_test = np.array([np.array(image) for image in mnist_test.data], dtype=np.float32)\n",
    "    y_test = np.array(mnist_test.targets, dtype=int)\n",
    "\n",
    "    # Normalize data to [0, 1]\n",
    "    X_train /= 255.0\n",
    "    X_test /= 255.0\n",
    "\n",
    "    # Reshape data from (60000, 28, 28) to (60000, 784) to match sklearn's format\n",
    "    X_train = X_train.reshape((X_train.shape[0], -1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], -1))\n",
    "\n",
    "    # Convert to JAX arrays\n",
    "    train_data = jnp.array(X_train)\n",
    "    test_data = jnp.array(X_test)\n",
    "    train_labels = jnp.array(y_train)\n",
    "    test_labels = jnp.array(y_test)\n",
    "\n",
    "    # Normalize using JAX\n",
    "    mean = jnp.mean(train_data, axis=0)\n",
    "    std = jnp.std(train_data, axis=0)\n",
    "    train_data = (train_data - mean) / (std + 1e-8)\n",
    "    test_data = (test_data - mean) / (std + 1e-8)\n",
    "\n",
    "    return train_data, train_labels, test_data, test_labels\n",
    "\n",
    "# Example usage\n",
    "train_data, train_labels, test_data, test_labels = load_mnist_local()\n",
    "print(train_data.shape, train_labels.shape, test_data.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494ad06a-45c8-4322-9336-1bdbe2faefb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_kan",
   "language": "python",
   "name": "venv_kan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
