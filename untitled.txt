
class MLP(nn.Module):
    def __init__(self, layers: Tuple[int, int, int], device: str):
        super().__init__()
        self.layer1 = nn.Linear(layers[0], layers[1], device=device)
        self.layer2 = nn.Linear(layers[1], layers[2], device=device)

    def forward(self, x: torch.Tensor):
        x = self.layer1(x)
        x = nn.functional.relu(x)
        x = self.layer2(x)
        x = nn.functional.sigmoid(x)
        return x






class EarlyStopping:
    def __init__(self, patience=5, min_delta=0):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = None
        self.early_stop = False

    def __call__(self, val_loss):
        if self.best_loss is None:
            self.best_loss = val_loss
        elif val_loss > self.best_loss - self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_loss = val_loss
            self.counter = 0





# Count parameters
def count_parameters(model):
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    return total_params, trainable_params







def run_experiment(tensor_dataset, valloader, model="MLP", dataset_used='MNIST', epochs=15, num_hidden=128, max_grad_norm=None, noise_multiplier=None, device='cpu', learning_rate=5e-3, batch_size=64, delta=1e-5):
  params = {
      'N': len(tensor_dataset),
      'l2_norm_clip': max_grad_norm,
      'noise_multiplier':  noise_multiplier,
      'minibatch_size': batch_size,
      'microbatch_size': 1,
      'delta': delta,
      'iterations': epochs * len(tensor_dataset) // batch_size,
      'lr': learning_rate,
  }


  # Define model
  # Calculate total and trainable parameters
    
  bool_flag = False # True # False

  if dataset_used == 'MNIST':
      input_dim = 28 * 28
      if model == "FasterKAN":
          model = FasterKAN([input_dim, num_hidden,  10], grid_min = -1.2, grid_max = 0.2, num_grids = 2, exponent = 2, inv_denominator = 0.5,        train_grid = bool_flag, train_inv_denominator = bool_flag).to(device)
      elif model == "MLP":
          model = MLP(layers=[input_dim, num_hidden, 10], device=device)
  elif dataset_used == 'CIFAR-10':
      input_dim = 3 * 32 * 32
      if model == "FasterKAN":
          model = FasterKAN([input_dim, 1, num_hidden, num_hidden // 2, num_hidden // 4, 10], grid_min=-1.2, grid_max=1.2, num_grids=64, exponent=2, inv_denominator=0.5, train_grid=False, train_inv_denominator=False).to(device)
      elif model == "MLP":
          model = MLP(layers=[input_dim, num_hidden, 10], device=device)
      elif model == "FasterKANvolver":
          model = FasterKANvolver([num_hidden * 2, num_hidden, num_hidden // 2, num_hidden // 4, 10], grid_min=-1.2, grid_max=1.2, num_grids=8, exponent=2, inv_denominator=0.5, train_grid=False, train_inv_denominator=False, view=[-1, 3, 32, 32], device=device).to(device)
      elif model == "MLPvolver":
          model = MLPvolver((input_dim, num_hidden, 10), view=[-1, 3, 32, 32], device=device).to(device)  # not really using input_dim


  total_params, trainable_params = count_parameters(model)
  print(f"Total parameters: {total_params}")
  print(f"Trainable parameters: {trainable_params}")

  model.to(device)

  # Define optimizer and scheduler
  if noise_multiplier is None:
    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-5)
  else:
    optimizer = optim.DPAdam(
        l2_norm_clip=params['l2_norm_clip'],
        noise_multiplier=params['noise_multiplier'],
        minibatch_size=params['minibatch_size'],
        microbatch_size=params['microbatch_size'],
        params=model.parameters(),
        lr=params['lr'],
  )

  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.6, patience=1, verbose=True)

  minibatch_loader, microbatch_loader = sampling.get_data_loaders(
        params['minibatch_size'],
        params['microbatch_size'],
        params['iterations']
  )


  # Define loss
  criterion = nn.CrossEntropyLoss()
  val_accuracy = 0

  model.train()
  for X_minibatch, y_minibatch in minibatch_loader(tensor_dataset):
      print('iteration', flush=True)
      optimizer.zero_grad()
      for X_microbatch, y_microbatch in microbatch_loader(TensorDataset(X_minibatch, y_minibatch)):
          if noise_multiplier != None:
              optimizer.zero_microbatch_grad()

          X_microbatch = X_microbatch.view(-1, input_dim).to(device)
          y_microbatch = y_microbatch.to(device)

          output = model(X_microbatch)
          loss = criterion(output, y_microbatch)
          loss.backward()
          if noise_multiplier != None:
              optimizer.microbatch_step()

      optimizer.step()

      accuracy = (output.argmax(dim=1) == y_microbatch).float().mean()


  # Validation
  model.eval()
  val_loss = 0
  val_accuracy = 0
  with torch.no_grad():
      for images, labels in valloader:
          images = images.view(-1, input_dim).to(device)
          output = model(images)
          val_loss += criterion(output, labels.to(device)).item()
          val_accuracy += (
              (output.argmax(dim=1) == labels.to(device)).float().mean().item()
          )
  val_loss /= len(valloader)
  val_accuracy /= len(valloader)

  if params['noise_multiplier'] == None:
      eps = None
  else:
      eps = analysis.epsilon(
                params['N'],
                params['minibatch_size'],
                params['noise_multiplier'],
                params['iterations'],
                params['delta']
                )
    
  return trainable_params, val_accuracy, eps, delta